---
title: "Partial Least Squares"
subtitle: "CMDA 4654 Project 2"
author: "Nicholas Emig, Eshan Kaul, Anish Manikonda, Andrew Nutter, Ashwanth Sridhar"
date: "Group 1"
output:
  xaringan::moon_reader:
    css: ["default", "assets/tech-fonts.css", "assets/tech.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
library(knitr)
```

```{r, load_refs, echo=FALSE, cache=FALSE, message=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
myBib <- ReadBib("assets/example.bib", check = FALSE)
top_icon = function(x) {
  icons::icon_style(
    icons::fontawesome(x),
    position = "fixed", top = 10, right = 10
  )
}
```

## Overview
* Background/Motivation

* PLS Algorithm

* Mtcars Example  

* Iris Example

* Sports Example

???
Hi everyone, my name is Ashwanth and today I am joined with my classmates Eshan, Anish, Andrew, and Nick to teach you guys about Partial Least Squares.

We will begin with a background and motivation, talk about theory and algorithms, and continue with examples using Mtcars, iris, and sports datasets.

---

## Background

* Partial Least Squares or projection to latent structures is a collection of regression based method designed for the analysis of high dimensional data in a low-structure environment. 

* Developed by econometrician Herman Wold in the late sixties

* Has use in many disciplines including bioinformatics, economics, social science, and **chemometrics**

* Many different variations exist:
.font90.pull-left[
* PLS-W2A
* PLS1
* PLS2
* PLS-SVD
* Mode B PLS (Canonical Correlation Analysis)
* PLS-R
* PLS-DA
* PLS-PM
  ]
.font90.pull-right[
* PLS-SEM
* OPLS
* L-PLS
* SPLS
* Nonlinear Iterative Partial Least Squares (NIPALS)
* SIMPLS
* KernalPLS
]

???

PLS uses a projection to latent structures which are uncorrelated or orthogonal, linear combinations of predictors used to analyze high dimensional data or data with many predictors in a low structure environment, making it similar to PCR, with some differences. This method was developed by Econometrician Herman World in the late 60s and is now used in disciplines such as psychology, economics, political science, and chemistry.

There are many different variations of PLS including PLS-SVD, PLS Canonical, PLS discriminant analysis, Orthogonal Projection PLS, Nonlinear Iterative PLS, and Kernel PLS, some of which will be discussed further in detail later in the presentation. The first four PLS options all use 1 principal component and have a univariate response. PLS Sem uses path mapping to identify principal components. PLS R uses regression methods. SIMPLS and KernalPLS are more efficient algorithms that identify principal components.


PLS-DA = Discriminant Analysis  
PLS-PM = Path Modeling  
PLS-SEM = Squares Path Modeling  
OPLS = Orthogonal Projections to Latent Structures  
SPLS = Sparse Partial Least Squares  

---

## Motivation

* The goal of PLS regression is to predict Y from X and describe their common structure.
* When $Y$ is a vector and $X$ is full rank this can be solved by MLR
* However problems with this arise when $X$ is likely to be singular
  * when the number of predictors/features is large compared to the number of observations 
  * when multicollinearity exists (feature matrix $X$ has less than full rank)  
[For detailed explanation see chapter 3](https://users.cecs.anu.edu.au/~kee/pls.pdf)

???
The main motivation for PLS is to predict Y from X and develop an understanding of their relation. Typically, when data is long, having few predictors and many observations, MLR or multiple linear regression can be used. However there are two major issues with this form of analysis. MLR cannot be used when there are fewer observations than predictors because this would create a rank deficient least squares problem and MLR also has issues while dealing with multicollinearity which is when different predictors have interaction with one another. To avoid dealing with these problems, PCR or PLS can be used.

---

## What is PLS

* Partial Least Squares (PLS) is a multivariate regression method
* The goal of PLS is to find a low-dimensional representation of the input data that captures the maximum covariance between the input and output variables
* PLS works by iteratively extracting pairs of latent variables or "components" that capture the maximum covariance between the input and output variables
* These components are then used to build a linear regression model
* PLS is a useful method for high-dimensional data with many predictor variables
* PLS can handle multicollinearity and non-linearity in the relationships between the predictor and response variables

???

Latent modeling uses linear combinations of predictors which are orthogonal to each other and try to find whichever combinations have the greatest covariance between the predictor and response. In PCR, it is assumed the input components are directly correlated with the output space. In PLS, the cross covariance is checked to ensure that the components are actually integral to the output space.

---

## Latent Modeling 

![](LatentModel1.svg)
![](LatentModel2.svg)

???
As mentioned in the previous slide, PCR uses only the input space to make predictions on the data. This is because PCR aims to maximize variance between the predictors. However, PLS uses information from the predictors and the response to maximize the cross covariance. Although they use different methods, both PLS and PCR often predict with the same level of accuracy. Generally though, PLS is able to use fewer components due to the cross-validation.

---

## PCR vs PLS

* PCR uses the $X$-information to build components
  * Choose basis vectors from low dimensional project that maximize variation of X
  $$\max\mathrm{Var}(X)$$
* PLS uses the $X$ and $Y$ information to build components by looking at the cross-covariance of $X$ and $Y$
  $$\max\mathrm{Cov}(X,Y)$$
* PLS and PCR often predict with similar performance & level of error
* PLS generally able to predict with fewer components  

---

## Visulization of PLS
![](geometric-interpretation-of-PLS-step1.png)

---

## Visulization of PLS

![](geometric-interpretation-of-PLS-step2.png)

---

## PLS Algorithm

Let $X \in \mathbb{R}^{n \times d}$ and $Y \in \mathbb{R}^{n \times t}$ be centered matrices, and $K$ is the number of components. Then for each $k \in [1, K]$:

1. Compute the first pair of singular vectors $X_k^T Y_k$. $u_k \in \mathbb{R}^d$ and $v_k \in \mathbb{R}^t$, the first left and right singular vectors of the cross-covariance matrix $C = X_k^T Y_k$.  

  1A.  Note $u_k$ and $v_k$ are weights that maximize the covariance between the $X_k$ and $\text{Cov}(X_k u_k, Y_k v_k)$  

2. Project $X_k$ and $Y_k$ on the singular vectors obtaining the scores $\xi_k = X_k u_k$ and $\omega_k = Y_k v_k$  

3. Obtain the rank-one approximations of the data matrices (loading vectors): 

  3A. Regress $X_k$ on $\xi_k$ to get $\gamma_k^T = (\xi_k^T \xi_k)^{-1}\, \xi_k^T X_k$  
  
  3B. Regress $Y_k$ on $\omega_k$ to get $\delta_k^T = (\omega_k^T \omega_k)^{-1}\, \omega_k^T Y_k$  

---

## PLS Algorithm Continued
  
<span>4.</span> Deflate $X_k$ and $Y_k$ by subtracting the rank-one approximations:   

&nbsp; &nbsp; 4A. $X_{k+1} = X_k - \xi_k \gamma_k^T$  
  
&nbsp; &nbsp; 4B. $Y_{k + 1} = Y_k - \omega_k \delta_k^T$  
  
<span>5.</span> If $X_{k+1}^T Y_{k + 1} = 0$  

&nbsp; &nbsp; 5A. $K \leftarrow k$ (Note this is the rank or dimension of the PLS model)  

&nbsp; &nbsp; 5B. Stop/Exit loop  

Else continue.  
  
<span>6.</span> $k \leftarrow k+1$  

<span>7.</span> Return to step 1

---

## PLS Algorithm Continued

In the end we have decomposed $\mathbf{X}$ and $\mathbf{Y}$ by expressing each as the sum of mutually orthogonal rank-one matrices, plus a residual:

$$\left.\begin{array}{rl}
\mathbf{X} & =\widehat{\mathbf{X}}^{(1)}\left(\boldsymbol{\xi}_1\right)+\ldots+\widehat{\mathbf{X}}^{(K)}\left(\boldsymbol{\xi}_K\right)+\mathbf{X}^{(K+1)} \\
& =\boldsymbol{\xi}_1 \gamma_1^T+\ldots+\boldsymbol{\xi}_K \gamma_K^T+\mathbf{X}^{(K+1)} \\
& =\Xi \Gamma^T+\mathbf{X}^{(K+1)} \\
\text { and } & \\
\mathbf{Y} & =\widehat{\mathbf{Y}}^{(1)}\left(\omega_1\right)+\ldots+\widehat{\mathbf{Y}}^{(K)}\left(\omega_K\right)+\mathbf{Y}^{(K+1)} \\
& =\omega_1 \delta_1^T+\ldots+\omega_K \boldsymbol{\delta}_K^T+\mathbf{Y}^{(K+1)} \\
& =\boldsymbol{\Omega} \Delta^T+\mathbf{Y}^{(K+1)}
\end{array}\right\}$$

---

## Kernel PLS Algorithm

1. **Preprocessing**: If `center = TRUE`, the mean of each column of `X` and `Y` is subtracted from the corresponding column. Otherwise, the data are used as is.  

2. **Initialize**: Set `i = 1`, initialize the residual matrices `E` and `F` as `X` and `Y`, respectively.  

3. **Calculate the kernel matrices**: Calculate the kernel matrix KX as the inner product between the preprocessed X data and itself, and the kernel matrix KY as the inner product between the preprocessed Y data and itself.  

4. **Calculate the scores**: Calculate the scores `T` and `U` as the normalized eigenvectors of the matrices `KX` and `KY`, respectively. Normalize by the eigenvalues.  

5. **Calculate the loadings**: Calculate the loadings `P` and Q as the normalized inner product between `X` and `T`, and `Y` and `U`, respectively.

---

## Kernel PLS Algorithm Continued

<span>6.</span> **Calculate the loading weights**: Calculate the loading weights `W` and `C` as the normalized eigenvectors of the matrix `X'PWY`, where `X'` is the transpose of `X`. Normalize by the eigenvalues.  

<span>7.</span> **Calculate the coefficient matrix**: Calculate the coefficient matrix `B` as the product of `W` and `C'`.  

<span>8.</span> **Update residual matrices**: Calculate the new residual matrices `E` and `F` as `E_new = E - TP'` and `F_new = F - UQ'`.  

<span>9.</span> **Check convergence**: Check the convergence criterion. If `i < ncomp` and the residual matrices `E` and `F` have not converged, go to step 3 with `i = i + 1`. Otherwise, stop.   

---

## Kernel vs NIPALS

* Kernel is much faster than NIPALS for large datasets where the number of observations (rows) is much larger than the number of observed (columns).  

* Kernel algorithm calculates the kernel matrix KX and KY only once and uses the eigendecomposition of these matrices to calculate the scores, loadings, loading weights, and coefficients.  

  * .font60[**Kernel trick** allows the algorithm to operate in a high-dimensional feature space without actually needing to compute the coordinates of the data in that space. Instead, the algorithm only needs to compute the inner product between pairs of data points in that space.]

* The NIPALS algorithm calculates these quantities iteratively for each component, which is computationally expensive for large datasets.  

* Kernal algorithm only deflates the Y-matrix while NIPALS deflates the X- and Y-matrices after each laten vector computation.  

---

## PLS on mtcars

Step-by-step demonstration on how to perform partial least squares on the built-in R dataset: mtcars

#### Step 1: Load 'pls' Package

```{r, message = FALSE}
# Load pls package
library(pls)

# View first six rows of mtcars dataset
kable(head(mtcars), format = "markdown")

```
---
## PLS on mtcars
	
#### Step 2: Fit Partial Least Squares model
	
The following code shows how to fit the PLS model to this data.

```{r}
# Make this example reproducible
set.seed(1)

# Fit PLS model
model <- plsr(hp ~ mpg + disp + drat + wt + qsec, 
              data = mtcars,
              scale = TRUE, 
              validation = "CV")
```
Note:

* scale=TRUE: Ensures that no predictor variable is overly influential in the model if it happens to be measured in different units.

* validation=”CV”: Used for k-fold cross-validation to evaluate the performance of the model. This uses k=10 folds by default.
---
## PLS on mtcars

#### Step 3: Choose the Number of PLS Components

```{r}
# View summary of model fitting
summary(model)
```
---
## PLS on mtcars

```{r, fig.align='center', out.width = "40%"}
# Visualize cross-validation plot
validationplot(model, val.type = "RMSEP", col = "blue", main = "RMSEP")
```
---
## PLS on mtcars

#### Step 4: Make Predictions using Final Model

```{r, results='hide'}
# Define training and testing sets
train <- mtcars[1:25, c("hp", "mpg", "disp", "drat", "wt", "qsec")]
y_test <- mtcars[26:nrow(mtcars), c("hp")]
test <- mtcars[26:nrow(mtcars), c("mpg", "disp", "drat", "wt", "qsec")]
    
# Use model to make predictions on a test set
model_pls <- plsr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")
model_pcr <- pcr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")

pls_pred <- predict(model_pls, test, ncomp=2)
pcr_pred <- predict(model_pcr, test, ncomp=2)

# Calculate RMSE
rmse_pls <- sqrt(mean((pls_pred - y_test)^2))
rmse_pcr <- sqrt(mean((pcr_pred - y_test)^2))
```
---
## PLS on mtcars

#### Results: 
```{r, echo=FALSE}
cat(" PLS test RMSE:", round(rmse_pls, 1),"\n",
   "PCR test RMSE:", round(rmse_pcr, 1), "\n")
```

* A lower RMSE value indicates better prediction performance. 
* Therefore, based on this specific dataset and model setup, the PLS model outperformed the PCR model with respect to predicting hp.

---


## PLS on iris

Suppose we want to predict the sepal length from all other variables.

$$Y = \text{Sepal width}$$
$$X_1 = \text{Petal length}$$
$$X_2 = \text{Petal length}$$
$$X_3 = \text{Petal width}$$

```{r baby}
head <- head(iris[1:4])
library(kableExtra)
head %>%
  kbl(caption = "Iris dataset") %>%
  kable_classic(full_width = F, html_font = "Cambria", position='float_left')
library(knitr)

MLR <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width, data=iris)
library(car)
vif <- t(vif(MLR))
vif %>%
  kbl(caption = "VIF values for MLR") %>%
  kable_classic(full_width = F, html_font = "Cambria", position='right')
```

---

## PLS on iris

#### Check data for multicolinearity

```{r colinearity, echo = FALSE, out.width='40%', out.extra='style="float:left; padding:20px"'}
library(GGally)
ggpairs(iris, columns = c('Sepal.Length', 'Sepal.Width',  'Petal.Length', 'Petal.Width'), upper = list(continuous = "smooth"), lower = list(continuous = "cor") )
```

Petal length and petal width appear to be highly correlated. 


#### Why should partial least squares be used here?

 - Partial Least Squares Regression is able to deal with multicolinearity by creating new variables called PLS components that are uncorrelated.
 - Unlike Principal Components Regression, PLS considers both the response and predictor variables for superior predictions.

---

## PLS on iris

Since multicolineariy is present, the iris dataset is a good candidate for partial least squares regression.

```{r PLS, echo = TRUE}
library(pls)
set.seed(1)
PLSmodel <- plsr(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width,
                 data=iris, scale=TRUE, validation="CV")
summary(PLSmodel)
```
---

## PLS on iris

### Finding the optimal model

```{r RMSE, echo = TRUE, fig.show="hold", out.width="25%"}
validationplot(PLSmodel, main='RMSEP vs components')
validationplot(PLSmodel, val.type="MSEP", main='MSEP vs components')
validationplot(PLSmodel, val.type="R2", main='R squared vs components')
```

As the number of components increases, root mean squared error decreases, Mean squared error prediction decreases, and R squared increases. Therefore all three components should be used.

---

## PLS on iris

### Creating the optimal model

```{r final, echo = TRUE, fig.show="hold", out.width="33%"}
# Create a training dataset
training <- iris[1:120, c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]

# Create a testing dataset for Y
yTest <- iris[121:nrow(iris), c('Sepal.Length')]

# Create a testing dataset for X
test <- iris[121:nrow(iris), c('Sepal.Width', 'Petal.Length', 'Petal.Width')]

# Build final model based off optimal component values
finalModel <- plsr(Sepal.Length~Sepal.Width+Petal.Length+Petal.Width,
                   data=training, scale=TRUE, validation='CV')

# Predict values and calculate RMSE
predictor <- predict(finalModel, test, ncomp=3)
RMSE <- sqrt(mean((predictor - yTest)^2))
```

The average deviation between the predicted sepal length and the actual observed sepal length is `r RMSE`.

---

## References
.font60[
* Abdi H, Williams LJ. Partial least squares methods: partial least squares correlation and partial least square regression. Methods Mol Biol. 2013;930:549-79. doi: 10.1007/978-1-62703-059-5_23. PMID: 23086857.

* Dayal, B., & MacGregor, J. (1997). Improved PLS algorithms. Journal of Chemometrics, 11(1), 73-85.

* Dunn, Kevin. “6.7 A Conceptual Explanation of PLS.” Process Improvement Using Data, 1 Feb. 2023, https://learnche.org/pid/latent-variable-modelling/projection-to-latent-structures/conceptual-mathematical-and-geometric-interpretation-of-pls. 

* Kristian Hovde Liland [aut, cre]. (2022, July 16). PLS: Partial least squares and principal component regression version 2.8-1 from cran. version 2.8-1 from CRAN. Retrieved April 24, 2023, from https://rdrr.io/cran/pls/ 

* Rosipal, R., Krämer, N. (2006). Overview and Recent Advances in Partial Least Squares. In: Saunders, C., Grobelnik, M., Gunn, S., Shawe-Taylor, J. (eds) Subspace, Latent Structure and Feature Selection. SLSFS 2005. Lecture Notes in Computer Science, vol 3940. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11752790_2

* Wegelin, J.A. (2000). A Survey of Partial Least Squares (PLS) Methods, with Emphasis on the Two-Block Case.

* 1.8. cross decomposition. scikit. (2023). Retrieved April 24, 2023, from https://scikit-learn.org/stable/modules/cross_decomposition.html#cross-decomposition 
]

